{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMA 869_Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8zRnHfXDfbQ"
      },
      "source": [
        "#Importing & Installing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JW1dLosUPo7"
      },
      "source": [
        "#Data Manipulation\n",
        "import numpy as np # recall that \"np\" etc. -- are abbreviated names we gave to these packages for notational convenience\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, confusion_matrix, make_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBrJ71DlSPG_",
        "outputId": "b9b7a530-fe13-4196-d65b-f0e6a78233c5"
      },
      "source": [
        "!pip install scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7qoDr-Ncv7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9bd183-e9cb-4d1b-a59f-63ca1beed578"
      },
      "source": [
        "#For FE and Data Modeling\n",
        "!pip install category_encoders\n",
        "!pip install flaml\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity= \"all\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.2.2\n",
            "Collecting flaml\n",
            "  Downloading FLAML-0.5.12-py3-none-any.whl (215 kB)\n",
            "\u001b[K     |████████████████████████████████| 215 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting catboost>=0.23\n",
            "  Downloading catboost-0.26.1-cp37-none-manylinux1_x86_64.whl (67.4 MB)\n",
            "\u001b[K\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 319, in run\n",
            "    reqs, check_supported_wheels=not options.target_dir\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 128, in resolve\n",
            "    requirements, max_rounds=try_to_avoid_resolution_too_deep\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 473, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 367, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_criteria_to_update(candidate)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 203, in _get_criteria_to_update\n",
            "    name, crit = self._merge_into_criterion(r, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _merge_into_criterion\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/structs.py\", line 139, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 129, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 33, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 205, in _make_candidate_from_link\n",
            "    version=version,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 312, in __init__\n",
            "    version=version,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 151, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 234, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 318, in _prepare_distribution\n",
            "    self._ireq, parallel_builds=True\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 552, in _prepare_linked_requirement\n",
            "    self.download_dir, hashes\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 243, in unpack_url\n",
            "    hashes=hashes,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/network/download.py\", line 157, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 156, in iter\n",
            "    self.next(len(x))  # noqa: B305\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/progress/__init__.py\", line 120, in next\n",
            "    self.update()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/progress/bar.py\", line 83, in update\n",
            "    self.writeln(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/progress/__init__.py\", line 101, in writeln\n",
            "    self.clearln()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/progress/__init__.py\", line 90, in clearln\n",
            "    print('\\r\\x1b[K', end='', file=self.file)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 106, in handle_sigint\n",
            "    self.finish()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 96, in finish\n",
            "    super().finish()  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/progress/__init__.py\", line 107, in finish\n",
            "    print(file=self.file)\n",
            "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 217, in _main\n",
            "    logger.critical(\"Exception:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1425, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 208, in format\n",
            "    msg = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 107, in handle_sigint\n",
            "    self.original_handler(signum, frame)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCgqEzpcGtcU"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import category_encoders as ce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op3Hg_JixjBT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns \n",
        "\n",
        "import itertools\n",
        "\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8RqIcVh5Tdp"
      },
      "source": [
        "# First we define a set of functions to compute the metrics of the model\n",
        "\n",
        "# ROC curve\n",
        "def plot_roc(y_test, y_pred):\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1, drop_intermediate = False)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    lw = 3\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "             lw=lw, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "    plt.plot([1, 2,3], [1,2,3], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.001, 1.001])\n",
        "    plt.ylim([-0.001, 1.001])\n",
        "    plt.xlabel('1-Specificity (False Negative Rate)')\n",
        "    plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "    plt.title('ROC curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Confusion Matrix returns in the format: cm[0,0], cm[0,1], cm[1,0], cm[1,1]: tn, fp, fn, tp\n",
        "\n",
        "# Sensitivity\n",
        "def custom_sensitivity_score(y_test, y_pred):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "    return (tp/(tp+fn))\n",
        "\n",
        "# Specificity\n",
        "def custom_specificity_score(y_test, y_pred):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "    return (tn/(tn+fp))\n",
        "\n",
        "# Positive Predictive Value\n",
        "def custom_ppv_score(y_test, y_pred):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "    return (tp/(tp+fp))\n",
        "\n",
        "# Negative Predictive Value\n",
        "def custom_npv_score(y_test, y_pred):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "    return (tn/(tn+fn))\n",
        "\n",
        "# Accuracy\n",
        "def custom_accuracy_score(y_test, y_pred):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "    return ((tn+tp)/(tn+tp+fn+fp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8vMnm0aDpR2"
      },
      "source": [
        "#Importing File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdvDbtMZdjEq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "04da9453-38df-4535-e83b-cf93f3d63916"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1yvNxI9wBle05flT_Y1AYDzuZ0aouytLY\"})\n",
        "downloaded.GetContentFile('training.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('training.csv', header=0) \n",
        "df.head()\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2a500b43b01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_application_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclear_output\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0m_gcloud_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0m_install_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mcolab_tpu_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36m_gcloud_login\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# https://github.com/jupyter/notebook/issues/3159\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mgcloud_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dufHHc37xXqo"
      },
      "source": [
        "#Feature Engineering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZuXp2h9xdTE"
      },
      "source": [
        "#Removing outliers, min-max for age, height..\n",
        "#Standardization of integers\n",
        "#Creating array for categorical features (hot-encoding)\n",
        "#IMP.Label encoding to a few categorical features should be done too, xgboost will love it...\n",
        "\n",
        "#Target encoding...\n",
        "\n",
        "#Can combine some features too.. * + - / #autofeat library\n",
        "\n",
        "#encoder= ce.MEstimateEncoder(cols=[])\n",
        "#encoder= ce.TargetEncoder\n",
        "\n",
        "#For Target encoding\n",
        "#import category_encoders as ce "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRKVHu2e19SX"
      },
      "source": [
        "import sklearn\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHzn7jNN1OVc"
      },
      "source": [
        "Data Cleaning - Turning strings into categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOppsQ2WCI6z"
      },
      "source": [
        "#Can do later to check..&Normalize..\n",
        "#scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(df[['geo_level_3_id']])\n",
        "#df['geo_level_3_id'] = scaler.transform(df[['geo_level_3_id']]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN4SFDiPCdwc"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbZ-utomxer6"
      },
      "source": [
        "df['land_surface_condition'] = df['land_surface_condition'].astype('category')\n",
        "df['foundation_type'] = df['foundation_type'].astype('category')\n",
        "df['roof_type'] = df['roof_type'].astype('category')\n",
        "df['ground_floor_type'] = df['ground_floor_type'].astype('category')\n",
        "df['position'] = df['position'].astype('category')\n",
        "df['plan_configuration'] = df['plan_configuration'].astype('category')\n",
        "df['other_floor_type'] = df['other_floor_type'].astype('category')\n",
        "df['legal_ownership_status'] = df['legal_ownership_status'].astype('category')\n",
        "\n",
        "df['damage_grade'] = df['damage_grade'].astype('category')\n",
        "\n",
        "df['has_superstructure_adobe_mud'] = df['has_superstructure_adobe_mud'].astype('category')\n",
        "df['has_superstructure_mud_mortar_stone'] = df['has_superstructure_mud_mortar_stone'].astype('category')\n",
        "df['has_superstructure_stone_flag'] = df['has_superstructure_stone_flag'].astype('category')\n",
        "df['has_superstructure_mud_mortar_brick'] = df['has_superstructure_mud_mortar_brick'].astype('category')\n",
        "df['has_superstructure_cement_mortar_brick'] = df['has_superstructure_cement_mortar_brick'].astype('category')\n",
        "df['has_superstructure_cement_mortar_stone'] = df['has_superstructure_cement_mortar_stone'].astype('category')\n",
        "df['has_superstructure_timber'] = df['has_superstructure_timber'].astype('category')\n",
        "df['has_superstructure_bamboo'] = df['has_superstructure_bamboo'].astype('category')\n",
        "df['has_superstructure_rc_non_engineered'] = df['has_superstructure_rc_non_engineered'].astype('category')\n",
        "df['has_superstructure_rc_engineered'] = df['has_superstructure_rc_engineered'].astype('category')\n",
        "df['has_superstructure_other'] = df['has_superstructure_other'].astype('category')\n",
        "df['has_secondary_use'] = df['has_secondary_use'].astype('category')\n",
        "df['has_secondary_use_agriculture'] = df['has_secondary_use_agriculture'].astype('category')\n",
        "df['has_secondary_use_hotel'] = df['has_secondary_use_hotel'].astype('category')\n",
        "df['has_secondary_use_rental'] = df['has_secondary_use_rental'].astype('category')\n",
        "df['has_secondary_use_institution'] = df['has_secondary_use_institution'].astype('category')\n",
        "df['has_secondary_use_school'] = df['has_secondary_use_school'].astype('category')\n",
        "df['has_secondary_use_industry'] = df['has_secondary_use_industry'].astype('category')\n",
        "df['has_secondary_use_health_post'] = df['has_secondary_use_health_post'].astype('category')\n",
        "df['has_secondary_use_gov_office'] = df['has_secondary_use_gov_office'].astype('category')\n",
        "df['has_secondary_use_use_police'] = df['has_secondary_use_use_police'].astype('category')\n",
        "df['has_secondary_use_other'] = df['has_secondary_use_other'].astype('category')\n",
        "\n",
        "df['geo_level_1_id'] = df['geo_level_1_id'].astype('category')\n",
        "df['geo_level_2_id'] = df['geo_level_2_id'].astype('category')\n",
        "df['geo_level_3_id'] = df['geo_level_3_id'].astype('category')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUhNsui4BgTG"
      },
      "source": [
        "#Some More EDA for Business Team"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvtRLVFyBpaY"
      },
      "source": [
        "selected_features = ['foundation_type', \n",
        "                     'area_percentage', \n",
        "                     'height_percentage',\n",
        "                     'count_floors_pre_eq',\n",
        "                     'land_surface_condition',\n",
        "                     'has_superstructure_cement_mortar_stone',\n",
        "                     'damage_grade']\n",
        "\n",
        "train_values_subset = df[selected_features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sveERv1WGk2"
      },
      "source": [
        "- \n",
        "\n",
        "1.   We can say area_percentage was left skewed, we did FE on it\n",
        "2.   The scatterplots show that damage grades is equally dis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdD8N8LoBvlb"
      },
      "source": [
        "sns.pairplot(train_values_subset, \n",
        "             hue='damage_grade')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4CytGB4GHox"
      },
      "source": [
        "#Package for visualizations\n",
        "!pip install -U yellowbrick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qk6Sd6JxO95"
      },
      "source": [
        "Testing & Training Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCIqqeLN5PF-"
      },
      "source": [
        "X = df.drop(columns = 'damage_grade')\n",
        "y = df['damage_grade']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Yfk87yGpKu"
      },
      "source": [
        "SHOWING THE IMBALANCE OF THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OrwsPbTt3eh"
      },
      "source": [
        "! pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1HLJE5tGd2A"
      },
      "source": [
        "#THERE IS IMBALANCE IN DATA AS SEEN BELOW #Damage grade\n",
        "\n",
        "from yellowbrick.datasets import load_game\n",
        "from yellowbrick.target import ClassBalance\n",
        "\n",
        "# Load the classification dataset\n",
        "X, y = df, df['damage_grade']\n",
        "\n",
        "# Instantiate the visualizer\n",
        "visualizer = ClassBalance(labels=[\"1\", \"2\", \"3\"])\n",
        "\n",
        "visualizer.fit(y)        # Fit the data to the visualizer\n",
        "visualizer.show()        # Finalize and render the figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXOQlMoAoVFE"
      },
      "source": [
        "X_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c7nrpEF2reZ"
      },
      "source": [
        "#Applying transformations to integer features..\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#Removing Outliers and standardizing\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_train[['age']])\n",
        "X_train['age'] = scaler.transform(X_train[['age']])\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_train[['height_percentage']])\n",
        "X_train['height_percentage'] = scaler.transform(X_train[['height_percentage']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_train[['area_percentage']])\n",
        "X_train['area_percentage'] = scaler.transform(X_train[['area_percentage']])   \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_train[['count_floors_pre_eq']])\n",
        "X_train['count_floors_pre_eq'] = scaler.transform(X_train[['count_floors_pre_eq']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_train[['count_families']])\n",
        "X_train['count_families'] = scaler.transform(X_train[['count_families']]) \n",
        "\n",
        "#Feature Engineering to the Testing Dataset...\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_test[['age']])\n",
        "X_test['age'] = scaler.transform(X_test[['age']])\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_test[['height_percentage']])\n",
        "X_test['height_percentage'] = scaler.transform(X_test[['height_percentage']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_test[['area_percentage']])\n",
        "X_test['area_percentage'] = scaler.transform(X_test[['area_percentage']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_test[['count_floors_pre_eq']])\n",
        "X_test['count_floors_pre_eq'] = scaler.transform(X_test[['count_floors_pre_eq']])\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(X_test[['count_families']])\n",
        "X_test['count_families'] = scaler.transform(X_test[['count_families']]) \n",
        "\n",
        "#Adding new Age*Height Feature...\n",
        "\n",
        "X_train['A_H'] = X_train['area_percentage'] * X_train['height_percentage']\n",
        "\n",
        "X_test['A_H'] = X_test['area_percentage'] * X_test['height_percentage']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtVolK-s6sph"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSwYlm-ijMRQ"
      },
      "source": [
        "#FE Using Clusters and Associations given by Data Robot.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7k6fuIo54w1"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1M_X0-0Qf9P"
      },
      "source": [
        "PLOTTING NUMERICAL VARIABLES TO CHECK DISTRIBUTION IS BETTER WITH NO OUTLIERS.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcVvBk8w6OJq"
      },
      "source": [
        "def plot_hist(ax, feature, title):\n",
        "    ax.hist(feature, bins=20, edgecolor='black', linewidth=1.2);\n",
        "    ax.set_title(title, fontsize=20);\n",
        "    ax.tick_params(axis='both', which='major', labelsize=18);\n",
        "    ax.grid(True);\n",
        "\n",
        "plt.figure(figsize=(16, 10));\n",
        "plt.grid(True);\n",
        "plot_hist(plt.subplot(3, 2, 1), X_train['age'], 'Age')\n",
        "plot_hist(plt.subplot(3, 2, 2), X_train['height_percentage'], 'Height')\n",
        "plot_hist(plt.subplot(3, 2, 3), X_train['area_percentage'], 'Area')\n",
        "plot_hist(plt.subplot(3, 2, 4), X_train['geo_level_1_id'], 'Region 1')\n",
        "plot_hist(plt.subplot(3, 2, 5), X_train['geo_level_2_id'], 'Region 2')\n",
        "plot_hist(plt.subplot(3, 2, 6), X_train['geo_level_3_id'], 'Region 3')\n",
        "\n",
        "#plt.title('Amount', fontsize=20)\n",
        "plt.tight_layout();\n",
        "#plt.savefig('out/german_credit-transforms.png');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ByXz-9VvA5"
      },
      "source": [
        "^ THIS IS TELLING YOU ABOUT THE FEAURES AFTER WE HAVE DONE PREPROCESSING - REMOVING OUTLIERS AND DISTRIBUTING THE DATA NORMALLY "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljBBloG-rv3"
      },
      "source": [
        "Categorical Feature Engineering using Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-jQgCtYGy68"
      },
      "source": [
        "categorical_features = df.select_dtypes(include=['category']).drop(['damage_grade'], axis=1).columns\n",
        "\n",
        "#CAN RUN THIS FOR FITTING MORE CATEGORICAL FEATURES IN TARGET ENCODING..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-RwmDhQW3mA"
      },
      "source": [
        "- TALK ABOUT THE FACT THAT WE RAN THE BELOW CODE TO LEARN WHICH TARGET ENCODING WOULD BE BEST FOR OUR DATA IN CLASSIFICATION MODELS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3LbAbvWHrVo"
      },
      "source": [
        "# Checking which target encoder is the best \n",
        "#!!!#DONT RUN THIS CODE NOTEBOOK WILL CRASH #I HAVE ALREADY DONE.\n",
        "\n",
        "encoder_list = [ce.hashing.HashingEncoder,\n",
        "                ce.helmert.HelmertEncoder,\n",
        "                ce.james_stein.JamesSteinEncoder,\n",
        "                ce.m_estimate.MEstimateEncoder,\n",
        "                ce.polynomial.PolynomialEncoder,\n",
        "                ce.woe.WOEEncoder\n",
        "                ]\n",
        "for encoder in encoder_list:\n",
        "    \n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('woe', encoder())])\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "    \n",
        "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', LGBMClassifier(n_estimators=2))])\n",
        "    \n",
        "    model = pipe.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    print(encoder)\n",
        "    print(f1_score(y_test, y_pred, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXXCkZlXJ76C"
      },
      "source": [
        "#ce_me = ce.MEstimateEncoder(cols = ['geo_level_1_id','geo_level_2_id','geo_level_3_id','land_surface_condition','foundation_type','roof_type','ground_floor_type','other_floor_type','position','plan_configuration','legal_ownership_status'],randomized=True)\n",
        "#ce_me.fit_transform(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQeaAjnlzfjO"
      },
      "source": [
        "#Targeting Econding on our Categorical Features\n",
        "#Category \n",
        "#Try reducing these target variables, may be causing target leakage in predictions..\n",
        "ce_me = ce.MEstimateEncoder(cols = ['geo_level_1_id','geo_level_2_id','land_surface_condition','foundation_type','roof_type','ground_floor_type','other_floor_type','position','plan_configuration','legal_ownership_status','has_superstructure_adobe_mud',\n",
        "                                    'has_superstructure_mud_mortar_stone','has_superstructure_rc_engineered','has_superstructure_rc_non_engineered','has_superstructure_cement_mortar_stone',\n",
        "                                    'has_secondary_use','has_superstructure_timber','has_superstructure_stone_flag','has_superstructure_cement_mortar_brick','has_superstructure_bamboo','has_secondary_use_industry','has_secondary_use_agriculture','has_secondary_use_rental','has_secondary_use_hotel','has_secondary_use_institution','has_secondary_use_school','has_secondary_use_health_post','has_secondary_use_gov_office','has_secondary_use_use_police','has_secondary_use_other'],m=2)\n",
        "\n",
        "X_train_fe=ce_me.fit_transform(X_train,y_train)\n",
        "X_test_fe=ce_me.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YrKsjJm0WAI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FncCAcr2YIi3"
      },
      "source": [
        "#ce_me = ce.MEstimateEncoder(cols = df[categorical_features],randomized=True,m=2)\n",
        "                            \n",
        "#X_train_fe = ce_me.fit_transform(X_train,y_train)\n",
        "#X_test_fe = ce_me.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpq-uf45uio2"
      },
      "source": [
        "X_train_fe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl9W58mUXT14"
      },
      "source": [
        "#Checking columns if left with zero variance or not... Not working properly though -_-  \n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "constant_filter = VarianceThreshold(threshold=0)\n",
        "\n",
        "constant_filter.fit(X_train_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj54SUygXkTr"
      },
      "source": [
        "constant_columns = [column for column in X_train_fe.columns\n",
        "                    if column not in X_train_fe.columns[constant_filter.get_support()]]\n",
        "\n",
        "print(len(constant_columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0HdDGmCpt7l"
      },
      "source": [
        "X_train_fe['F1'] = X_train_fe['has_secondary_use'] + X_train_fe['has_secondary_use_agriculture'] + X_train_fe['has_secondary_use_gov_office'] + X_train_fe['has_secondary_use_health_post'] + X_train_fe['has_secondary_use_industry'] + X_train_fe['has_secondary_use_use_police'] + X_train_fe['has_secondary_use_institution']\n",
        "\n",
        "X_train_fe['F2'] = X_train_fe['height_percentage'] * X_train_fe['count_floors_pre_eq']\n",
        "\n",
        "X_train_fe['F3'] = X_train_fe['has_superstructure_cement_mortar_brick'] + X_train_fe['has_superstructure_mud_mortar_stone']\n",
        "\n",
        "X_train_fe['F4'] = X_train_fe['has_superstructure_bamboo'] + X_train_fe['has_superstructure_timber']\n",
        "\n",
        "X_train_fe['F5'] = X_train_fe['ground_floor_type'] + X_train_fe['has_superstructure_cement_mortar_brick']\n",
        "\n",
        "X_train_fe['F6'] = X_train_fe['foundation_type'] + X_train_fe['has_superstructure_rc_engineered']\n",
        "\n",
        "X_train_fe['F7'] = X_train_fe['height_percentage'] + X_train_fe['other_floor_type']\n",
        "\n",
        "X_train_fe['F8'] = X_train_fe['foundation_type'] * X_train_fe['ground_floor_type']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqHId7sFt5CP"
      },
      "source": [
        "X_train_fe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r469qcXHWFnb"
      },
      "source": [
        "Applying Same Feature Engineering to the Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Jv8MWRo5aL"
      },
      "source": [
        "#Dropping columns with 0 variation in training set\n",
        "X_train_fe = X_train_fe.drop(columns = ['has_superstructure_bamboo','has_superstructure_stone_flag','has_superstructure_mud_mortar_brick','has_superstructure_cement_mortar_stone','has_superstructure_other','has_superstructure_rc_engineered','has_secondary_use_industry','has_secondary_use_agriculture','has_secondary_use_rental','has_secondary_use_hotel','has_secondary_use_institution','has_secondary_use_school','has_secondary_use_health_post','has_secondary_use_gov_office','has_secondary_use_use_police','has_secondary_use_other'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAHpXe-FVm5N"
      },
      "source": [
        "X_test_fe['F2'] = X_test_fe['height_percentage'] * X_test_fe['count_floors_pre_eq']\n",
        "\n",
        "X_test_fe['F3'] = X_test_fe['has_superstructure_cement_mortar_brick'] + X_test_fe['has_superstructure_mud_mortar_stone']\n",
        "\n",
        "X_test_fe['F4'] = X_test_fe['has_superstructure_bamboo'] + X_test_fe['has_superstructure_timber']\n",
        "\n",
        "X_test_fe['F5'] = X_test_fe['ground_floor_type'] + X_test_fe['has_superstructure_cement_mortar_brick']\n",
        "\n",
        "X_test_fe['F6'] = X_test_fe['foundation_type'] + X_test_fe['has_superstructure_rc_engineered']\n",
        "\n",
        "X_test_fe['F7'] = X_test_fe['height_percentage'] + X_test_fe['other_floor_type']\n",
        "\n",
        "X_test_fe['F8'] = X_test_fe['foundation_type'] * X_test_fe['ground_floor_type']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkK4CKFDM_tA"
      },
      "source": [
        "#Dropping columns with 0 variation in testing set\n",
        "X_test_fe = X_test_fe.drop(columns = ['has_superstructure_bamboo','has_superstructure_stone_flag','has_superstructure_mud_mortar_brick','has_superstructure_cement_mortar_stone','has_superstructure_other','has_superstructure_rc_engineered','has_secondary_use_industry','has_secondary_use_agriculture','has_secondary_use_rental','has_secondary_use_hotel','has_secondary_use_institution','has_secondary_use_school','has_secondary_use_health_post','has_secondary_use_gov_office','has_secondary_use_use_police','has_secondary_use_other'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaVgFe0dWFQ2"
      },
      "source": [
        "messi= X_train_fe.select_dtypes(include='number')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFa3xlzkWQ6s"
      },
      "source": [
        "corr = messi.corr()\n",
        "sns.heatmap(corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sILGH-OShu8L"
      },
      "source": [
        "X_train_fe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzAsCmOr1PYE"
      },
      "source": [
        "from yellowbrick.features import Rank2D\n",
        "\n",
        "visualizer = Rank2D(algorithm=\"pearson\")\n",
        "visualizer.fit_transform(X_train_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDzCPFQbIbrU"
      },
      "source": [
        "#@title\n",
        "#numeric_features = adults_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "#categorical_features = adults_data.select_dtypes(include=['object']).drop(['income'], axis=1).columns\n",
        "\n",
        "#X = adults_data.drop('income', axis=1)\n",
        "#y = adults_data['income']\n",
        "\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF8ghpREut8F"
      },
      "source": [
        "THE FOLLOWING PIPELINE CODE IS TELLING US WHICH CATEGORY ENCODING IS BEST FOR A SIMPLE RANDOM FOREST MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv-NkSu7u5Jw"
      },
      "source": [
        "FINDING BEST MODEL USING FLAML "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4XSCYYFghw8"
      },
      "source": [
        "#MODELING "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhBQ4qTmDlmP"
      },
      "source": [
        "from flaml import AutoML\n",
        "automl = AutoML()\n",
        "\n",
        "#FLAML\n",
        "settings = {\n",
        "    \"metric\": 'micro_f1',  # primary metrics can be chosen from: ['accuracy','roc_auc','f1','log_loss','mae','mse','r2']\n",
        "    \"task\": 'classification',  # task type    \n",
        "    \"log_file_name\": 'earthquake_experiment.log',\n",
        "    \"eval_method\":'auto',\n",
        "    \"model_history\":True,\n",
        "}\n",
        "\n",
        "#It checks ensemble models for you... "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNyAnij19m1w"
      },
      "source": [
        "automl.fit(X_train=X_train_fe, y_train=y_train,sample_weight_val='balanced', **settings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIn-0Ev98Pep"
      },
      "source": [
        "**- While running the FLAML AutoML algorithm, LGBmodel came out to be the best one with the best error on 0.2467 when it ran different models such as XGboost, Random Forests, Extra-tree & Cat-boost. This was achieved with 62 iterations. The Best parameter is given below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVoOdyxx_hLR"
      },
      "source": [
        "# Error score...0.2398 (Best running model for Data Driven)\n",
        "# To beat uncle steve we need 0.22 or 0.2250 #beat steve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJH66q9rG6iT"
      },
      "source": [
        "automl.model.estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T3pCl0rq2pK"
      },
      "source": [
        "#With 0.2433 Error\n",
        "\n",
        "#LGBMClassifier(colsample_bytree=0.9930577047295022,\n",
        "               learning_rate=0.10653852350625627, max_bin=256,\n",
        "               min_child_samples=18, n_estimators=29, num_leaves=125,\n",
        "               objective='multiclass', reg_alpha=0.0009765625,\n",
        "               reg_lambda=1.8287241464057549, subsample=0.9025475651868093)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKT37HxgHA0z"
      },
      "source": [
        "''' retrieve best config and best learner'''\n",
        "print('Best ML leaner:', automl.best_estimator)\n",
        "print('Best hyperparmeter config:', automl.best_config)\n",
        "print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
        "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
        "\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtuDyCls82km"
      },
      "source": [
        "#automl.fit(X_train, y_train, metric='micro_f1', task=\"classification\", estimator_list=['lgbm', 'xgboost', 'catboost', 'rf', 'extra_tree'], eval_method='cv',n_splits=5, model_history=True, log_training_metric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9AZ5UfqoUp7"
      },
      "source": [
        "#PREDICTING ON THE TESTING SET\n",
        "y_pred_lgbm = automl.predict(X_test_fe)\n",
        "\n",
        "y_pred_prob_lgbm=automl.predict_proba(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV7u6oAfvI0b"
      },
      "source": [
        "#@title\n",
        "print(y_pred_lgbm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q_Z9mWNpmyO"
      },
      "source": [
        "MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY15QlXQplIF"
      },
      "source": [
        "#@title\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test, y_pred_lgbm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-0QCvFdpwe0"
      },
      "source": [
        "#@title\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_lgbm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP698M4xQORT"
      },
      "source": [
        "#DONT RUN THIS TOO :) \n",
        "#FEATURE SELECTION\n",
        "#If want to improve more on the model...\n",
        "#from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "#sel=SelectKBest(chi2,k='all')\n",
        "#sel=sel.fit(X_train_fe,y_train)\n",
        "\n",
        "#X_train_new = sel.transform(X_train_fe)\n",
        "#X_test_new = sel.transform(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzh1gRjk9IYJ"
      },
      "source": [
        "MANUALLY RUNNING THE LGBM MODEL FOR CLASSIFICATION VISUALIZATIONS. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhBCMlLyBx5Y"
      },
      "source": [
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZQMXEoFKofO"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(colsample_bytree=0.9930577047295022,\n",
        "               learning_rate=0.10653852350625627, max_bin=256,\n",
        "               min_child_samples=18, n_estimators=29, num_leaves=125,\n",
        "               objective='multiclass', reg_alpha=0.0009765625,\n",
        "               reg_lambda=1.8287241464057549, subsample=0.9025475651868093)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1Dtvcs0Hxu"
      },
      "source": [
        "lgbm = LGBMClassifier(colsample_bytree=0.7146015935547059,\n",
        "               learning_rate=0.18085024148873766, max_bin=16,\n",
        "               min_child_samples=30, n_estimators=84, num_leaves=64,\n",
        "               objective='multiclass', reg_alpha=0.0012223292933999925,\n",
        "               reg_lambda=0.12134695755253136, subsample=0.8413048297641477)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nadU9kFAdXw"
      },
      "source": [
        "lgbm.fit(X_train_fe, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7R9gFzwMm4R"
      },
      "source": [
        "test_lgbm = lgbm.predict(X_test_fe)\n",
        "\n",
        "test_prob_lgbm=lgbm.predict_proba(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtm11RqQOD9l"
      },
      "source": [
        "class_names = [str(x) for x in lgbm.classes_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt0Xrs5Sz0_z"
      },
      "source": [
        "#Visualization of the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ms5OuqPAUB"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, log_loss\n",
        "\n",
        "print(\"Accuracy = {:.2f}\".format(accuracy_score(y_test, y_pred_lgbm)))\n",
        "print(\"Kappa = {:.2f}\".format(cohen_kappa_score(y_test, test_lgbm)))\n",
        "print(\"F1 Score = {:.2f}\".format(f1_score(y_test, test_lgbm,average='micro')))\n",
        "print(\"Log Loss = {:.2f}\".format(log_loss(y_test, test_prob_lgbm)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOFkuVhh5g-a"
      },
      "source": [
        "# Lets look at the model metrics\n",
        "\n",
        "print('Metrics of the Light Gradient Boosting Model: \\n')\n",
        "\n",
        "cm = np.transpose(confusion_matrix(y_test, test_lgbm))\n",
        "print(\"Confusion matrix: \\n\" + str(cm))\n",
        "\n",
        "print(\"                                   Accuracy: \" + str(custom_accuracy_score(y_test, test_lgbm))) \n",
        "print(\"                   SENSITIVITY (aka RECALL): \" + str(custom_sensitivity_score(y_test, test_lgbm)))\n",
        "print(\"                 SPECIFICITY (aka FALL-OUT): \" + str(custom_specificity_score(y_test, test_lgbm)))\n",
        "print(\" POSITIVE PREDICTIVE VALUE, (aka PRECISION): \" + str(custom_ppv_score(y_test, test_lgbm)))\n",
        "print(\"                 NEGATIVE PREDICTIVE VALUE): \" + str(custom_npv_score(y_test, test_lgbm)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkRlQy0sBLGM"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjrEOVz0Xsc9"
      },
      "source": [
        "#INTERPRET THE VISUALIZATION OF MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vtdGTR1Ge-H"
      },
      "source": [
        "# Visualizing LGBM Model\n",
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(lgbm,force_model=True)\n",
        "\n",
        "visualizer.fit(X_train_fe, y_train)  # Fit the visualizer and the model\n",
        "visualizer.score(X_test_fe, y_test)  # Evaluate the model on the test data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtW5nmFrEh-k"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "cm = ConfusionMatrix(lgbm)\n",
        "\n",
        "cm.score(X_test_fe, y_test)\n",
        "\n",
        "cm.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chehHKjs7KXr"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from yellowbrick.classifier import ROCAUC\n",
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "\n",
        "model = lgbm\n",
        "visualizer = ROCAUC(model)\n",
        "\n",
        "visualizer.fit(X_train_fe, y_train)        # Fit the training data to the visualizer\n",
        "visualizer.score(X_test_fe, y_test)        # Evaluate the model on the test data\n",
        "visualizer.show()                       # Finalize and show the figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNLVSw-_OmQC"
      },
      "source": [
        "labels = lgbm.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc5s3ZNhK66x"
      },
      "source": [
        "#How good the model has predicted class..\n",
        "from yellowbrick.classifier import ClassPredictionError\n",
        "\n",
        "visualizer = ClassPredictionError(lgbm)\n",
        "\n",
        "visualizer.fit(X_train_fe, y_train)\n",
        "visualizer.score(X_test_fe, y_test)\n",
        "g = visualizer.poof()\n",
        "\n",
        "#Problem of damage grade 2 with our dataset...\n",
        "#We still have not \n",
        "#class_weight='balanced' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeHQkNA3IHV8"
      },
      "source": [
        "Out of 40k, 30k were predicted correctly for class 2. The others were wrong and involved class 1 and 2 #Deep waters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha1ZPI8cfVRh"
      },
      "source": [
        "#Run this later\n",
        "from yellowbrick.classifier import PrecisionRecallCurve\n",
        "\n",
        "viz = PrecisionRecallCurve(lgbm);\n",
        "viz.fit(X_train_fe, y_train);\n",
        "viz.score(X_test_fe, y_test);\n",
        "viz.poof();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxYWPu6paWW_"
      },
      "source": [
        "!pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcofANg00C4S"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from yellowbrick.model_selection import FeatureImportances\n",
        "\n",
        "visualizer = FeatureImportances(GradientBoostingClassifier(),relative=False, topn=10)\n",
        "visualizer.fit(X_train_fe, y_train)\n",
        "visualizer.show()\n",
        "\n",
        "#F8 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7huQNZgbLKT"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOVjj8bNVarw"
      },
      "source": [
        "#@title\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA0LUduOUb6F"
      },
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "submit = pd.read_csv('test_values.csv', header=0) \n",
        "\n",
        "submit.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um91dIUiEUQh"
      },
      "source": [
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['geo_level_3_id']])\n",
        "submit['geo_level_3_id'] = scaler.transform(submit[['geo_level_3_id']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['geo_level_2_id']])\n",
        "submit['geo_level_2_id'] = scaler.transform(submit[['geo_level_2_id']])\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['geo_level_1_id']])\n",
        "submit['geo_level_1_id'] = scaler.transform(submit[['geo_level_1_id']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEXTYi-M1v3v"
      },
      "source": [
        "#@title\n",
        "submit['land_surface_condition'] = submit['land_surface_condition'].astype('category')\n",
        "submit['foundation_type'] = submit['foundation_type'].astype('category')\n",
        "submit['roof_type'] = submit['roof_type'].astype('category')\n",
        "submit['ground_floor_type'] = submit['ground_floor_type'].astype('category')\n",
        "submit['position'] = submit['position'].astype('category')\n",
        "submit['plan_configuration'] = submit['plan_configuration'].astype('category')\n",
        "submit['other_floor_type'] = submit['other_floor_type'].astype('category')\n",
        "submit['legal_ownership_status'] = submit['legal_ownership_status'].astype('category')\n",
        "\n",
        "\n",
        "submit['has_superstructure_adobe_mud'] = submit['has_superstructure_adobe_mud'].astype('category')\n",
        "submit['has_superstructure_mud_mortar_stone'] = submit['has_superstructure_mud_mortar_stone'].astype('category')\n",
        "submit['has_superstructure_stone_flag'] = submit['has_superstructure_stone_flag'].astype('category')\n",
        "submit['has_superstructure_mud_mortar_brick'] = submit['has_superstructure_mud_mortar_brick'].astype('category')\n",
        "submit['has_superstructure_cement_mortar_brick'] = submit['has_superstructure_cement_mortar_brick'].astype('category')\n",
        "submit['has_superstructure_cement_mortar_stone'] = submit['has_superstructure_cement_mortar_stone'].astype('category')\n",
        "submit['has_superstructure_timber'] = submit['has_superstructure_timber'].astype('category')\n",
        "submit['has_superstructure_bamboo'] = submit['has_superstructure_bamboo'].astype('category')\n",
        "submit['has_superstructure_rc_non_engineered'] = submit['has_superstructure_rc_non_engineered'].astype('category')\n",
        "submit['has_superstructure_rc_engineered'] = submit['has_superstructure_rc_engineered'].astype('category')\n",
        "submit['has_superstructure_other'] = submit['has_superstructure_other'].astype('category')\n",
        "submit['has_secondary_use'] = submit['has_secondary_use'].astype('category')\n",
        "submit['has_secondary_use_agriculture'] = submit['has_secondary_use_agriculture'].astype('category')\n",
        "submit['has_secondary_use_hotel'] = submit['has_secondary_use_hotel'].astype('category')\n",
        "submit['has_secondary_use_rental'] = submit['has_secondary_use_rental'].astype('category')\n",
        "submit['has_secondary_use_institution'] = submit['has_secondary_use_institution'].astype('category')\n",
        "submit['has_secondary_use_school'] = submit['has_secondary_use_school'].astype('category')\n",
        "submit['has_secondary_use_industry'] = submit['has_secondary_use_industry'].astype('category')\n",
        "submit['has_secondary_use_health_post'] = submit['has_secondary_use_health_post'].astype('category')\n",
        "submit['has_secondary_use_gov_office'] = submit['has_secondary_use_gov_office'].astype('category')\n",
        "submit['has_secondary_use_use_police'] = submit['has_secondary_use_use_police'].astype('category')\n",
        "submit['has_secondary_use_other'] = submit['has_secondary_use_other'].astype('category')\n",
        "\n",
        "submit['geo_level_1_id'] = submit['geo_level_1_id'].astype('category')\n",
        "submit['geo_level_2_id'] = submit['geo_level_2_id'].astype('category')\n",
        "submit['geo_level_3_id'] = submit['geo_level_3_id'].astype('category')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd03nmI4r_mM"
      },
      "source": [
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['age']])\n",
        "submit['age'] = scaler.transform(submit[['age']])\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['height_percentage']])\n",
        "submit['height_percentage'] = scaler.transform(submit[['height_percentage']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['area_percentage']])\n",
        "submit['area_percentage'] = scaler.transform(submit[['area_percentage']]) \n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['count_floors_pre_eq']])\n",
        "submit['count_floors_pre_eq'] = scaler.transform(submit[['count_floors_pre_eq']])\n",
        "\n",
        "scaler = preprocessing.FunctionTransformer(np.log1p, validate=True).fit(submit[['count_families']])\n",
        "submit['count_families'] = scaler.transform(submit[['count_families']]) \n",
        "\n",
        "submit['A_H'] = submit['area_percentage'] * submit['height_percentage']\n",
        "\n",
        "submit['A_H'] = submit['area_percentage'] * submit['height_percentage']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HinZUZhY7Lo_"
      },
      "source": [
        "ce_me = ce.MEstimateEncoder(cols = ['geo_level_1_id','geo_level_2_id','land_surface_condition','foundation_type','roof_type','ground_floor_type','other_floor_type','position','plan_configuration','legal_ownership_status','has_superstructure_adobe_mud',\n",
        "                                    'has_superstructure_mud_mortar_stone','has_superstructure_rc_engineered','has_superstructure_rc_non_engineered','has_superstructure_cement_mortar_stone',\n",
        "                                    'has_secondary_use','has_superstructure_timber','has_superstructure_stone_flag','has_superstructure_cement_mortar_brick','has_superstructure_bamboo','has_secondary_use_industry','has_secondary_use_agriculture','has_secondary_use_rental','has_secondary_use_hotel','has_secondary_use_institution','has_secondary_use_school','has_secondary_use_health_post','has_secondary_use_gov_office','has_secondary_use_use_police','has_secondary_use_other'],m=2)\n",
        "\n",
        "X_train_fe=ce_me.fit_transform(X_train,y_train)\n",
        "predict_fe=ce_me.transform(submit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-__OdIfD5Vt"
      },
      "source": [
        "ce_me = ce.MEstimateEncoder(cols = ['has_superstructure_bamboo','has_superstructure_stone_flag','has_superstructure_mud_mortar_brick','has_superstructure_cement_mortar_stone','has_superstructure_other','has_superstructure_rc_engineered','has_secondary_use_industry','has_secondary_use_agriculture','has_secondary_use_rental','has_secondary_use_hotel','has_secondary_use_institution','has_secondary_use_school','has_secondary_use_health_post','has_secondary_use_gov_office','has_secondary_use_use_police','has_secondary_use_other'],m=2)\n",
        "\n",
        "X_train_fe=ce_me.fit_transform(X_train,y_train)\n",
        "predict_fe=ce_me.transform(submit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRKNTJMuGke_"
      },
      "source": [
        "\n",
        "\n",
        "predict_fe['F2'] = predict_fe['height_percentage'] * predict_fe['count_floors_pre_eq']\n",
        "\n",
        "predict_fe['F7'] = predict_fe['height_percentage'] + predict_fe['other_floor_type']\n",
        "\n",
        "predict_fe['F8'] = predict_fe['foundation_type'] * predict_fe['ground_floor_type'] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZZf5UuEEAXz"
      },
      "source": [
        "predict_fe['F3'] = predict_fe['has_superstructure_cement_mortar_brick'] * predict_fe['has_superstructure_mud_mortar_stone']\n",
        "\n",
        "predict_fe['F4'] = predict_fe['has_superstructure_bamboo'] + predict_fe['has_superstructure_timber']\n",
        "\n",
        "predict_fe['F5'] = predict_fe['ground_floor_type'] + predict_fe['has_superstructure_cement_mortar_brick']\n",
        "\n",
        "predict_fe['F6'] = predict_fe['foundation_type'] + predict_fe['has_superstructure_rc_engineered']\n",
        "\n",
        "predict_fe['F1'] = predict_fe['has_secondary_use'] + predict_fe['has_secondary_use_agriculture'] + predict_fe['has_secondary_use_gov_office'] + predict_fe['has_secondary_use_health_post'] + predict_fe['has_secondary_use_industry'] + predict_fe['has_secondary_use_use_police'] + predict_fe['has_secondary_use_institution']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbuWVkk29cXH"
      },
      "source": [
        "predict_fe = predict_fe.drop(columns = ['has_superstructure_bamboo','has_superstructure_stone_flag','has_superstructure_mud_mortar_brick','has_superstructure_cement_mortar_stone','has_superstructure_other','has_superstructure_rc_engineered','has_secondary_use_industry','has_secondary_use_agriculture','has_secondary_use_rental','has_secondary_use_hotel','has_secondary_use_institution','has_secondary_use_school','has_secondary_use_health_post','has_secondary_use_gov_office','has_secondary_use_use_police','has_secondary_use_other'])\n",
        "predict_fe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuTAdISG3D9D"
      },
      "source": [
        "#Predicting using lgbm \n",
        "y_submit = automl.predict(predict_fe)\n",
        "\n",
        "y_submit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmOZaVnA4WtM"
      },
      "source": [
        "#@title\n",
        "submission = pd.DataFrame({\n",
        "        \"building_id\": submit[\"building_id\"],\n",
        "        \"damage_grade\": y_submit\n",
        "    })\n",
        "\n",
        "submission.to_csv('mySubmission_lgbm17.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEJO578FIpHy"
      },
      "source": [
        "Can do for Feature Selection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYnJ0o1_IT0s"
      },
      "source": [
        "#@title\n",
        "#If want to improve more on the model...\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "#sel=SelectKBest(chi2,k=35)\n",
        "sel=sel.fit(X_train,y_train)\n",
        "\n",
        "#X_train_new = sel.transform(X_train)\n",
        "X_test_new = sel.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWyX5A51JuNj"
      },
      "source": [
        "#@title\n",
        "# Need to create code for the model\n",
        "# Need to type in Cross validation code..\n",
        "# Need to type hyperparameter tuning with it too\n",
        "\n",
        "#run cv for every combo of hyperparameter tuning (searh.results for the best...)\n",
        "\n",
        "#retrain on the best combination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHTrLlXoq9-e"
      },
      "source": [
        "#Random Forest (Ensemble)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShEH9vHozOfI"
      },
      "source": [
        "from flaml import AutoML\n",
        "automl2 = AutoML()\n",
        "\n",
        "\n",
        "settings = {\n",
        "    \"metric\": 'micro_f1',  # primary metrics can be chosen from: ['accuracy','roc_auc','f1','log_loss','mae','mse','r2']\n",
        "    \"task\": 'classification',  # task type    \n",
        "    \"log_file_name\": 'earthquake_experiment.log',\n",
        "    \"eval_method\":'auto',\n",
        "    \"model_history\":True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4HdEGsaM0ET"
      },
      "source": [
        "automl2.fit(X_train_fe, y_train, estimator_list=[\"rf\"],**settings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6JsnwqjW30j"
      },
      "source": [
        "automl2.model.estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHiD6eqOP5O"
      },
      "source": [
        "''' retrieve best config and best learner'''\n",
        "print('Best ML leaner:', automl2.best_estimator)\n",
        "print('Best hyperparmeter config:', automl2.best_config)\n",
        "print('Best accuracy on validation data: {0:.4g}'.format(1-automl2.best_loss))\n",
        "print('Training duration of best run: {0:.4g} s'.format(automl2.best_config_train_time))\n",
        "\n",
        "print('Best hyperparmeter config:', automl2.best_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBFLo7W5Uo39"
      },
      "source": [
        "#PREDICTING ON THE TESTING SET\n",
        "y_pred_rf = automl2.predict(X_test_fe)\n",
        "\n",
        "y_pred_prob_rf=automl2.predict_proba(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2EbubMUVyC2"
      },
      "source": [
        "MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOwT8RUoVxJl"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test, y_pred_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z45x2hpV-8b"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKJMrsTQWM1v"
      },
      "source": [
        "Manually creating RF Using Grid Search CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niUMBsUN-8AX"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param = {'n_estimators': [ 500, 1000,2500], \n",
        "'min_samples_split':[20, 50, 500],\n",
        "'max_leaf_nodes':[50,100,150]}\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "gd_sr = GridSearchCV(estimator=clf,param_grid=param,scoring='f1_micro',cv=5,n_jobs=-1)\n",
        "\n",
        "gd_sr.fit(X_train_fe,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds8r40iEWvuO"
      },
      "source": [
        "# Define a random forest model and call it classifier_RF\n",
        "classifier_RF = RandomForestClassifier(criterion='entropy', max_features=0.6142013843610947,\n",
        "                       max_leaf_nodes=316, n_estimators=8, n_jobs=-1) # recall what random_state mean\n",
        "\n",
        "# Train the model classifier_RF on the training data\n",
        "classifier_RF.fit(X_train_fe, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIzgKTsSXRmG"
      },
      "source": [
        "test_rf = classifier_RF.predict(X_test_fe)\n",
        "\n",
        "test_prob_rf=classifier_RF.predict_proba(X_test_fe)\n",
        "\n",
        "class_names = [str(x) for x in classifier_RF.classes_]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, log_loss\n",
        "\n",
        "print(\"Accuracy = {:.2f}\".format(accuracy_score(y_test, test_rf)))\n",
        "print(\"Kappa = {:.2f}\".format(cohen_kappa_score(y_test, test_rf)))\n",
        "print(\"F1 Score = {:.2f}\".format(f1_score(y_test, test_rf,average='micro')))\n",
        "print(\"Log Loss = {:.2f}\".format(log_loss(y_test, test_prob_rf)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziJR4vyMYHiX"
      },
      "source": [
        "# Lets look at the model metrics\n",
        "\n",
        "print('Metrics of the Random Forest model: \\n')\n",
        "\n",
        "cm = np.transpose(confusion_matrix(y_test, test_rf))\n",
        "print(\"Confusion matrix: \\n\" + str(cm))\n",
        "\n",
        "print(\"                                   Accuracy: \" + str(custom_accuracy_score(y_test, test_rf))) \n",
        "print(\"                   SENSITIVITY (aka RECALL): \" + str(custom_sensitivity_score(y_test, test_rf)))\n",
        "print(\"                 SPECIFICITY (aka FALL-OUT): \" + str(custom_specificity_score(y_test, test_rf)))\n",
        "print(\" POSITIVE PREDICTIVE VALUE, (aka PRECISION): \" + str(custom_ppv_score(y_test, test_rf)))\n",
        "print(\"                 NEGATIVE PREDICTIVE VALUE): \" + str(custom_npv_score(y_test, test_rf)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TujjG13T6EzU"
      },
      "source": [
        "# Visualizing LGBM Model\n",
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(classifier_RF,class_names)\n",
        "\n",
        "visualizer.fit(X_train_fe, y_train)  # Fit the visualizer and the model\n",
        "visualizer.score(X_test_fe, y_test)  # Evaluate the model on the test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXIVhyeh6QAo"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from yellowbrick.classifier import ROCAUC\n",
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "\n",
        "model = classifier_RF\n",
        "visualizer = ROCAUC(model)\n",
        "\n",
        "visualizer.fit(X_train_fe, y_train)        # Fit the training data to the visualizer\n",
        "visualizer.score(X_test_fe, y_test)        # Evaluate the model on the test data\n",
        "visualizer.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxHHup96a3d"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from yellowbrick.datasets import load_occupancy\n",
        "from yellowbrick.model_selection import FeatureImportances\n",
        "\n",
        "\n",
        "model = classifier_RF\n",
        "\n",
        "visualizer = FeatureImportances((model),relative=False, topn=10)\n",
        "visualizer.fit(X_train_fe, y_train)\n",
        "visualizer.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlejPtJuWug_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWsE6nx1e0G"
      },
      "source": [
        "#XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2syYq3ZPbBJ7"
      },
      "source": [
        "from flaml import AutoML\n",
        "automl3 = AutoML()\n",
        "\n",
        "\n",
        "settings = {\n",
        "    \"metric\": 'micro_f1',  # primary metrics can be chosen from: ['accuracy','roc_auc','f1','log_loss','mae','mse','r2']\n",
        "    \"task\": 'classification',  # task type    \n",
        "    \"log_file_name\": 'earthquake_experiment.log',\n",
        "    \"eval_method\":'auto',\n",
        "    \"model_history\":True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgsShqE9a_Wj"
      },
      "source": [
        "automl3.fit(X_train_fe, y_train, estimator_list=[\"'xgboost\"],**settings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpcvpiHrKi88"
      },
      "source": [
        "automl3.model.estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2-V-ECrYRO9"
      },
      "source": [
        "#PREDICTING ON THE TESTING SET\n",
        "y_pred_xgbm = automl3.predict(X_test_fe)\n",
        "\n",
        "y_pred_prob_xgbm=automl3.predict_proba(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLT6Coki8mC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150109f9-7127-4e95-db24-f3f69c16cad5"
      },
      "source": [
        "pip install xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGadQ_Mj8uII"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWFkVbgg9hJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "8e37ffd6-d2b9-4ff2-f9ca-8a1d10621674"
      },
      "source": [
        "from xgboost import XGBMClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e0ff3ae80529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'XGBMClassifier' from 'xgboost' (/usr/local/lib/python3.7/dist-packages/xgboost/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLJGfJhb7e_7"
      },
      "source": [
        "xgbm= xgb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oryzm8oC7qjQ"
      },
      "source": [
        "lgbm = LGBMClassifier(colsample_bytree=0.9930577047295022,\n",
        "               learning_rate=0.10653852350625627, max_bin=256,\n",
        "               min_child_samples=18, n_estimators=29, num_leaves=125,\n",
        "               objective='multiclass', reg_alpha=0.0009765625,\n",
        "               reg_lambda=1.8287241464057549, subsample=0.9025475651868093)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m8KfEQV7Ym0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgbm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JmtRrf7clv7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIwIi8R7zq9I"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "\n",
        "def visualize_model(X, y, estimator, **kwargs):\n",
        "    \"\"\"\n",
        "    Test various estimators.\n",
        "    \"\"\"\n",
        "    y = LabelEncoder().fit_transform(y)\n",
        "    model = Pipeline([\n",
        "        ('one_hot_encoder', OneHotEncoder()),\n",
        "        ('estimator', estimator)\n",
        "    ])\n",
        "\n",
        "    # Instantiate the classification model and visualizer\n",
        "    visualizer = ClassificationReport(\n",
        "        model, classes=['edible', 'poisonous'],\n",
        "        cmap=\"YlGn\", size=(600, 360), **kwargs\n",
        "    )\n",
        "    visualizer.fit(X, y)\n",
        "    visualizer.score(X, y)\n",
        "    visualizer.show()\n",
        "\n",
        "for model in models:\n",
        "    visualize_model(X, y, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISsvxJLNamRe"
      },
      "source": [
        "# Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXVeKsPLaqXq"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42, criterion=\"entropy\",\n",
        "                             min_samples_split=10, min_samples_leaf=10, max_depth=3, max_leaf_nodes=5,class_weight='balanced')\n",
        "clf.fit(X_train_fe, y_train)\n",
        "\n",
        "y_pred_dt = clf.predict(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb59r592cHUK"
      },
      "source": [
        "class_names = [str(x) for x in clf.classes_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXKp8tDMcMRW"
      },
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_dt, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtIq0XwIdZEX"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, log_loss\n",
        "\n",
        "print(\"Accuracy = {:.2f}\".format(accuracy_score(y_test, y_pred_dt)))\n",
        "print(\"Kappa = {:.2f}\".format(cohen_kappa_score(y_test, y_pred_dt)))\n",
        "print(\"F1 Score = {:.2f}\".format(f1_score(y_test, y_pred_dt,average='micro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t44Cwsktek9x"
      },
      "source": [
        "#Bigger and prettier!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 7));\n",
        "\n",
        "tree.plot_tree(clf, filled=True, feature_names=feature_names, class_names=class_names, proportion=False, fontsize=12);\n",
        "\n",
        "#plt.savefig('out/marketing-dt-tree.png', transparent=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joHysSUievhz"
      },
      "source": [
        "from yellowbrick.classifier import ROCAUC\n",
        "\n",
        "visualizer = ROCAUC(clf)\n",
        "\n",
        "visualizer.fit(X_train_fe, y_train)  # Fit the training data to the visualizer\n",
        "visualizer.score(X_test_fe, y_test)  # Evaluate the model on the test data\n",
        "g = visualizer.poof() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXvGUlFNfuWc"
      },
      "source": [
        "\n",
        "class_names = [str(x) for x in clf.classes_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzlTergGhF2A"
      },
      "source": [
        "feature_names = ['Age', 'Income']\n",
        "X = df[feature_names].to_numpy()\n",
        "y = df['Bought'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb2JWeGHfhyE"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import tree\n",
        "tree.plot_tree(clf)\n",
        "plt.figure(figsize=(12, 7));\n",
        "tree.plot_tree(clf, filled=True, proportion=False, fontsize=12);\n",
        "\n",
        "#plt.savefig('out/marketing-dt-tree.png', transparent=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vdi2IatiLDK"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAUaVbFXiMnX"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_clf.fit(X_train_fe, y_train)\n",
        "\n",
        "y_pred_knn = knn_clf.predict(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvPLntGGi4us"
      },
      "source": [
        "\n",
        "print(\"Accuracy = {:.2f}\".format(accuracy_score(y_test, y_pred_knn)))\n",
        "print(\"Kappa = {:.2f}\".format(cohen_kappa_score(y_test, y_pred_knn)))\n",
        "print(\"F1 Score = {:.2f}\".format(f1_score(y_test, y_pred_knn,average='micro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpuefyrrjaYJ"
      },
      "source": [
        "#SVM Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m92iHr4jcl0"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC(kernel=\"linear\", C=0.025)\n",
        "svm_clf.fit(X_train_fe, y_train)\n",
        "\n",
        "y_pred_svm = svm_clf.predict(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5zfOxxLkA8d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkr_sgLjkRNN"
      },
      "source": [
        "# Nueral networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H_52Ir-j_uB"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn_clf = MLPClassifier(solver='lbfgs', activation='relu', alpha=1e-3, max_iter=1500,\n",
        "                       hidden_layer_sizes=(10, 10), random_state=1, verbose=True)\n",
        "nn_clf.fit(X_train_fe, y_train)\n",
        "\n",
        "y_pred_nn = nn_clf.predict(X_test_fe)\n",
        "nn_clf.predict_proba(X_test_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpAUJs43kW3i"
      },
      "source": [
        "print(classification_report(y_test, y_pred_nn, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx-1DMlGkc-v"
      },
      "source": [
        "\n",
        "print(\"Accuracy = {:.2f}\".format(accuracy_score(y_test, y_pred_nn)))\n",
        "print(\"Kappa = {:.2f}\".format(cohen_kappa_score(y_test, y_pred_nn)))\n",
        "print(\"F1 Score = {:.2f}\".format(f1_score(y_test, y_pred_nn)))\n",
        "print(\"Log Loss = {:.2f}\".format(log_loss(y_test, y_pred_nn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-coD-Y5kXvJ"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hucHihzUkbEX"
      },
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_clf = LogisticRegression(random_state=22)\n",
        "lr_clf.fit(X_train_fe, y_train)\n",
        "\n",
        "y_pred_lr = lr_clf.predict(X_test_fe)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr, target_names=class_names))\n",
        "\n",
        "print(\"Accuracy = {:.2f}\".format(accuracy_score(y_test, y_pred_lr)))\n",
        "print(\"Kappa = {:.2f}\".format(cohen_kappa_score(y_test, y_pred_lr)))\n",
        "print(\"F1 Score = {:.2f}\".format(f1_score(y_test, y_pred_lr)))\n",
        "print(\"Log Loss = {:.2f}\".format(log_loss(y_test, y_pred_lr)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}